{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Applications of Machine Learning with Artificial Neural Networks and Supervised Regression</h1></center>\n",
    "<br>\n",
    "<center><i>Lucas Barbosa</i></center><hr>\n",
    "<h3>Training Time</h3>\n",
    "<p>Neural networks are like lawyers, useless unless taught how to do good hence the training algorithms have to be pretty good to round up a decent combination of weights. The most optimal way as mentioned before is using a cost function, that cost function again being</p><br>\n",
    "\n",
    "<center>$ \\eta = \\sum \\frac{1}{2} (y - \\hat{y})^{2} $</center><br>\n",
    "\n",
    "<p>The one half coefficient is there to omit the power when differentiation the cost function later for simplicity. The squared also exploits the <strong>convex</strong> nature of the cost function to avoid having to use Stochastic Gradient Descent as opposed to Batch Gradient Descent to train the ANN.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td><center><strong>Stochastic Gradient Descent</strong><br>(SGD)</center></td>\n",
    "            <td><center><strong>Batch Gradient Descent</strong><br>(BGD)</center></td>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Error is computed randomly through each individual input and its corresponding output’s error.</td>\n",
    "            <td>Error is computed through a summation of the total error of all the inputs and corresponding outputs.</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The advantage to using Gradient Descent is that it doesn’t matter how many dimensions our data has, the <strong>curse of dimensionality</strong> doesn’t effect the overall algorithm in anyway since it knows which was is right, which way convergence lies. The only thing that actually needs to be done is for an algorithm to be written which computes the gradient of the cost function in respects to both the weights W1 and W2 being used. The way to compute all these numbers is to start with the initial error which is in our output, then work backward from there hence why applying Gradient Descent to ANN’s is referred to <strong>back propagation</strong>.</p>\n",
    "\n",
    "<p>Since computing the gradient of the cost function involves more than one term (both weights) partial derivatives must be taken in respect to each weight <strong>individually</strong> then applied individually too. Since we’re working backwards here the first term in respect to the cost will be W2. The gradient descent algorithm will be derived using the <strong>chain rule</strong> in calculus, which makes it much simples to connect all the dots.</p><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(2)}} = \\sum \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W^{(2)}} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(2)}} = \\sum \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} $</center><br>\n",
    "\n",
    "<p>where:</p>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial \\hat{y}} = -(y - \\hat{y}) $</center><br>\n",
    "\n",
    "<p>As yhat is the result of the activation function on z3, its rate of change will equate to its derivative.</p>\n",
    "\n",
    "<center>$ \\hat{y} = \\varphi(z^{(3)}) $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} = {\\varphi}'(z^{(3)}) $</center><br>\n",
    "\n",
    "<p>This is where z3 was a product of a2 and W2.</p>\n",
    "\n",
    "<center>$ z^{(3)} = a^{(2)} W^{(2)} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a^{(2)} $</center><br>\n",
    "\n",
    "<p>For the sake of simplity, concider:</p>\n",
    "\n",
    "<center>$ \\delta^{(3)} = -(y - \\hat{y}) \\ {\\varphi}'(z^{(3)})  $</center><br>\n",
    "\n",
    "<p>This putting all of the parts of the puzzle together and transposing matrix a2 to make sure all of its entries are mutiplied by delta3 to yield the total back propagation error for the second set of weights.</p>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(2)}} = \\sum -(y - \\hat{y}) \\ \\varphi(z^{(3)}) \\ (a^{(2)})^{T} $</center>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(2)}} = \\sum \\delta^{(3)} \\ (a^{(2)})^{T} $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The same partial derivative needs to be computed for the first set of weights W1. The same approach with the chain rule will be taken, however there will be some different aspects to it.</p><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W^{(1)}} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\frac{\\partial z^{(3)}}{\\partial W^{(1)}} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} $</center><br>\n",
    "\n",
    "<p>where:</p>\n",
    "\n",
    "<center>$ \\delta^{(3)} = \\frac{\\partial \\eta}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z^{(3)}} $</center>\n",
    "\n",
    "<p>Making sure things stay simple and kept in as few terms as possible:</p>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(3)} \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} $</center>\n",
    "\n",
    "<p>where also:</p>\n",
    "\n",
    "<center>$ z^{(3)} = a^{(2)} W^{(2)}  $</center>\n",
    "\n",
    "<center>$ \\frac{\\partial z^{(3)}}{\\partial a^{(2)}} = W^{(2)} $</center>\n",
    "\n",
    "<p>For the last piece of the puzzle, In order to make sense of the rest, the activity of the second layer must be used to connect to the first set of weights W1.</p>\n",
    "\n",
    "<center>$ \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} = \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} $</center><br>\n",
    "\n",
    "<p>Similiar to the sigmoid prime of z3 above, the same differentiating happens with a2 and z2.</p>\n",
    "\n",
    "<center>$ a^{(2)} = \\varphi(z^{(2)}) $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = {\\varphi}'(z^{(2)})  $</center><br>\n",
    "\n",
    "<p>The final term has arrived all the way back to the beginning of the network, the X input matrix.</p>\n",
    "\n",
    "<center>$ z^{(2)} = X W^{(1)} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} = X $</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To bring it all together now in terms of W1:</p>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(3)} \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} (W^{(2)})^{T} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(3)} {\\varphi}'(z^{(2)})  \\frac{\\partial z^{(2)}}{\\partial W^{(1)}} (W^{(2)})^{T} $</center><br>\n",
    "\n",
    "<p>Both the X and W2 matrices required to be <strong>transposed</strong> in order for their respective entries to be successfully multiplied by every value of delta3 and delta2, the back propagating error of each layer of weights.</p><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(3)} {\\varphi}'(z^{(2)}) X^T (W^{(2)})^{T} $</center><br>\n",
    "\n",
    "<p>where:</p>\n",
    "\n",
    "<center>$ \\delta^{(2)} =  \\delta^{(3)} {\\varphi}'(z^{(2)}) (W^{(2)})^{T} $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(2)} X^T  $</center><br>\n",
    "\n",
    "<p>Therefore the algorithms which will compute the respective gradients of the weights W1 and W2 ultimately evaluate to:</p><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(1)}} = \\sum \\delta^{(2)} X^T $</center><br>\n",
    "\n",
    "<center>$ \\frac{\\partial \\eta}{\\partial W^{(2)}} = \\sum \\delta^{(3)} \\ (a^{(2)})^{T} $</center><br>\n",
    "\n",
    "<p>Now that the algorithms are clear and ready to be put in use its time to code them up. These Gradient Descent computations will be implemented into the previous version of the Neural Network class. A new feature added to this class is the regularization parameter used to make sure that the data isn't <strong>overfitted</strong> and still models the real world.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self, learning_rate=0):\n",
    "        # define hyperparameters\n",
    "        self.input_layer_size = 2\n",
    "        self.hidden_layer_size = 3\n",
    "        self.output_layer_size = 1\n",
    "        \n",
    "        # define parameters\n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "        # regularization parameter\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # forward propagation\n",
    "    def forward(self, X):\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        prediction = self.sigmoid(self.z3)\n",
    "        return prediction\n",
    "    \n",
    "    # activation functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # derivative of sigmoid function\n",
    "    def sigmoid_prime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "    \n",
    "    # efficient backprop\n",
    "    def cost_function(self, X, desired_output):\n",
    "        self.prediction = self.forward(X)\n",
    "        total_error = ((1/2) * sum((desired_output - self.prediction)**2)) / X.shape[0] + \\\n",
    "                      (self.learning_rate / 2) * (np.sum(self.W1**2) + np.sum(self.W2**2))\n",
    "        return total_error\n",
    "            \n",
    "    def cost_function_prime(self, X, desired_y):\n",
    "        self.prediction = self.forward(X)\n",
    "        \n",
    "        # layer 3 backprop error\n",
    "        l3_backprop_error   = np.multiply(-(desired_y - self.prediction), \\\n",
    "                              self.sigmoid_prime(self.z3))\n",
    "        # divide by X.shape[0] to account for the scale of the data \n",
    "        cost_in_terms_of_W2 = np.dot(self.a2.T, l3_backprop_error) / X.shape[0] + \\\n",
    "                              (self.learning_rate * self.W2)\n",
    "            \n",
    "        # layer 2 backprop error\n",
    "        l2_backprop_error   = np.dot(l3_backprop_error, self.W2.T) * \\\n",
    "                              self.sigmoid_prime(self.z2)\n",
    "        # divide by X.shape[0] to account for the scale of the data\n",
    "        cost_in_terms_of_W1 = np.dot(X.T, l2_backprop_error) / X.shape[0] + \\\n",
    "                              (self.learning_rate * self.W1)\n",
    "                \n",
    "        return cost_in_terms_of_W1, cost_in_terms_of_W2\n",
    "    \n",
    "    # altering and setting the parameters during training\n",
    "    def get_params(self):\n",
    "        # get W1 & W2 rolled into a vector\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        # set W1 & W2 using single parameter vector\n",
    "        W1_start = 0\n",
    "        W1_end   = self.hidden_layer_size * self.input_layer_size\n",
    "        # reshape the W1 weights \n",
    "        self.W1  = np.reshape(params[W1_start : W1_end], \\\n",
    "                   (self.input_layer_size, self.hidden_layer_size))\n",
    "        W2_end   = W1_end + self.hidden_layer_size * self.output_layer_size\n",
    "        # reshape the W2 weights\n",
    "        self.W2  = np.reshape(params[W1_end : W2_end], \\\n",
    "                   (self.hidden_layer_size, self.output_layer_size))\n",
    "        \n",
    "    def compute_gradient(self, X, desired_y):\n",
    "        cost_in_terms_of_W1, cost_in_terms_of_W2 = self.cost_function_prime(X, desired_y)\n",
    "        return np.concatenate((cost_in_terms_of_W1.ravel(), cost_in_terms_of_W2.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before any further progress in terms of training the network is done one more thing is crucial. It’s not all the time that the ANN’s algorithms actually work and converge to nice set of weight parameters. In fact some of the time there could be an underlying issue which involves the incorrect calculation towards a gradient. The problem with this type of error is that it won’t stop the network from still propagating, so there’s no way to know if something’s wrong…unless there’s a way of making sure that the gradients being outputted are at least slightly accurate. This is achieved through <strong>Numerical Gradient Checking</strong>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Helper(object):\n",
    "    def __init__(self, Local_Ref):\n",
    "        # set a local reference to NN class\n",
    "        self.Local_Ref = Local_Ref\n",
    "       \n",
    "    # normalize data to account for different units\n",
    "    def scale_data(self, hours, test_score):\n",
    "        MAX_SCORE = 100.\n",
    "        hours      /= np.amax(hours, axis=0)\n",
    "        test_score /= MAX_SCORE\n",
    "        return hours, test_score \n",
    "    \n",
    "    # checking gradients with numerical gradient computation avoiding logic errors\n",
    "    def compute_numerical_gradient(self, X, desired_y):\n",
    "        initial_params     = self.Local_Ref.get_params()\n",
    "        numerical_gradient = np.zeros(initial_params.shape)\n",
    "        perturb            = np.zeros(initial_params.shape)\n",
    "        \n",
    "        # epsilon value needs to be small enough act as a 'zero'\n",
    "        epsilon = 1e-4   \n",
    "        \n",
    "        for i in range(len(initial_params)):\n",
    "            # set perturbation vector to alter the original state of the initial params\n",
    "            perturb[i] = epsilon\n",
    "            self.Local_Ref.set_params(initial_params + perturb)\n",
    "            loss_2 = self.Local_Ref.cost_function(X, desired_y)\n",
    "            \n",
    "            self.Local_Ref.set_params(initial_params - perturb)\n",
    "            loss_1 = self.Local_Ref.cost_function(X, desired_y)\n",
    "            \n",
    "            # computer numerical gradient\n",
    "            numerical_gradient[i] = (loss_2 - loss_1) / (2 * epsilon)\n",
    "            \n",
    "            perturb[i] = 0\n",
    "        \n",
    "        self.Local_Ref.set_params(initial_params)\n",
    "        return numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# training data\n",
    "train_x = np.array(([3,5],[5,1],[10,2],[6,1.5]), dtype=float)\n",
    "train_y = np.array(([75],[82],[93],[70]), dtype=float)\n",
    "\n",
    "# testing data\n",
    "test_x = np.array(([4, 5.5],[4.5, 1],[9,2.5],[6,2]), dtype=float)\n",
    "test_y = np.array(([70],[89],[85],[75]), dtype=float)\n",
    "\n",
    "NN = Neural_Network(learning_rate=0.0001)\n",
    "Aux = Helper(NN)\n",
    "\n",
    "# normalize data\n",
    "train_x, train_y = Aux.scale_data(train_x, train_y)\n",
    "test_x, test_y   = Aux.scale_data(test_x, test_y)\n",
    "\n",
    "# check to see gradients have been correctly calculated\n",
    "numerical_gradient = Aux.compute_numerical_gradient(train_x, train_y)\n",
    "computed_gradient  = NN.compute_gradient(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00371666, -0.01351013,  0.00163045,  0.00236209, -0.00863253,\n",
       "        0.00106616, -0.02461633, -0.02341835, -0.03432765])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00371666, -0.01351013,  0.00163045,  0.00236209, -0.00863253,\n",
       "        0.00106616, -0.02461633, -0.02341835, -0.03432765])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The numerical gradient values seem to be very much the same to the computed gradients values which indicate that our ANN is actually working. Now that the gradients are actually being outputed its time to write up a Trainer class for this ANN. The trainer class will also incorporate the <strong>Quasi-Newton BFGS Mathematical Optimization</strong> algorithm to change it up a little.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, Local_Ref):\n",
    "        # make local reference to NN\n",
    "        self.Local_Ref = Local_Ref\n",
    "        \n",
    "    def cost_function_wrapper(self, params, X, desired_y):\n",
    "        self.Local_Ref.set_params(params)\n",
    "        total_cost = self.Local_Ref.cost_function(X, desired_y)\n",
    "        gradient   = self.Local_Ref.compute_gradient(X, desired_y)\n",
    "        return total_cost, gradient\n",
    "    \n",
    "    # track cost function value as training progresses\n",
    "    def callback(self, params):\n",
    "        self.Local_Ref.set_params(params)\n",
    "        self.cost_list.append(self.Local_Ref.cost_function(self.train_x, self.train_y))\n",
    "        self.test_cost_list.append(self.Local_Ref.cost_function(self.test_x, self.test_y))\n",
    "    \n",
    "    def train(self, train_x, train_y, test_x, test_y):\n",
    "        \n",
    "        # internal variable for callback function\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "        self.test_x = test_x\n",
    "        self.test_y = test_y\n",
    "        \n",
    "        # empty lists to store costs\n",
    "        self.cost_list = []\n",
    "        self.test_cost_list = []\n",
    "        \n",
    "        initial_params =  self.Local_Ref.get_params()\n",
    "        \n",
    "        options = {\"maxiter\": 200, \"disp\": True}\n",
    "        _result = optimize.minimize(self.cost_function_wrapper, initial_params, jac=True, \\\n",
    "                                    method=\"BFGS\", args=(train_x, train_y), options=options, \\\n",
    "                                    callback=self.callback)\n",
    "        \n",
    "        # once the training is complete finally set the new values in\n",
    "        self.Local_Ref.set_params(_result.x)\n",
    "        self.optimization_results = _result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002618\n",
      "         Iterations: 99\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 101\n"
     ]
    }
   ],
   "source": [
    "T = Trainer(NN)\n",
    "T.train(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x110faf6d8>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAF5CAYAAACIpbAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1106ad0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# graphing the cost in respect to interations\n",
    "plt.plot(T.cost_list)\n",
    "plt.plot(T.test_cost_list)\n",
    "plt.grid(1)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75],\n",
       "       [ 0.82],\n",
       "       [ 0.93],\n",
       "       [ 0.7 ]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74737269],\n",
       "       [ 0.78120047],\n",
       "       [ 0.85748338],\n",
       "       [ 0.80043227]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.forward(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Thanks to the early <strong>regularisation hyper parameter</strong> there is no <i>overfitting</i> of data and the training and testing data still model the real world. This just about concludes  the training process of the ANN and in fact the life span of this project.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
